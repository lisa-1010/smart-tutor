{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sketchbook to experiment with RNN models to model the dynamics. Once tested, the code in this notebook will be incorporated into functions/classes/ python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Model\n",
    "Models the dynamics of the system p(next observation | history of observations, action)\n",
    "\n",
    "History of observations consist of exercises a student has done and whether the student solved each of them\n",
    "\n",
    "Action is the next exercise chosen \n",
    "\n",
    "Next observation is whether the student gets the chosen exercise correct\n",
    "\n",
    "\n",
    "We want to use an RNN to model the dynamics.\n",
    "Input data represents history of observations, of shape (n_students, n_timesteps, observation_vec_size)\n",
    "\n",
    "Output represents the probability of getting next exercise correctly, of shape (n_students, n_timesteps, n_exercises) \n",
    "\n",
    "So at each timestep, we make a prediction for all actions. \n",
    "\n",
    "For each action, the output vector specifies the predicted probability of the student getting the chosen exercise correctly.\n",
    "\n",
    "The target output only contains binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lisa1010/tf_venv/bin/python\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.executable\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset_utils.load_data(filename=\"../synthetic_data/toy.pickle\")\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_options {\n",
       "}\n",
       "allow_soft_placement: true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflearn.init_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "n_samples, n_timesteps, n_inputdim = input_data_.shape\n",
    "_,_,n_outputdim = target_data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print n_inputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print n_outputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 2PTSBL\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "---------------------------------\n",
      "Training samples: 4\n",
      "Validation samples: 1\n",
      "--\n",
      "Training Step: 1  | time: 2.116s\n",
      "| Adam | epoch: 001 | loss: 0.00000 | val_loss: 0.04748 -- iter: 4/4\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.04031\u001b[0m\u001b[0m | time: 1.067s\n",
      "| Adam | epoch: 002 | loss: 0.04031 | val_loss: 0.04669 -- iter: 4/4\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.04343\u001b[0m\u001b[0m | time: 1.065s\n",
      "| Adam | epoch: 003 | loss: 0.04343 | val_loss: 0.04589 -- iter: 4/4\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.04341\u001b[0m\u001b[0m | time: 1.061s\n",
      "| Adam | epoch: 004 | loss: 0.04341 | val_loss: 0.04506 -- iter: 4/4\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.04293\u001b[0m\u001b[0m | time: 1.060s\n",
      "| Adam | epoch: 005 | loss: 0.04293 | val_loss: 0.04419 -- iter: 4/4\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.04231\u001b[0m\u001b[0m | time: 1.056s\n",
      "| Adam | epoch: 006 | loss: 0.04231 | val_loss: 0.04326 -- iter: 4/4\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.04168\u001b[0m\u001b[0m | time: 1.061s\n",
      "| Adam | epoch: 007 | loss: 0.04168 | val_loss: 0.04223 -- iter: 4/4\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.04099\u001b[0m\u001b[0m | time: 1.055s\n",
      "| Adam | epoch: 008 | loss: 0.04099 | val_loss: 0.04111 -- iter: 4/4\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.04024\u001b[0m\u001b[0m | time: 1.059s\n",
      "| Adam | epoch: 009 | loss: 0.04024 | val_loss: 0.03985 -- iter: 4/4\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.03943\u001b[0m\u001b[0m | time: 1.053s\n",
      "| Adam | epoch: 010 | loss: 0.03943 | val_loss: 0.03846 -- iter: 4/4\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "with graph_to_use.as_default():\n",
    "    net = tflearn.input_data([None, n_timesteps, n_inputdim],dtype=tf.float32, name='input_data')\n",
    "    output_mask = tflearn.input_data([None, n_timesteps, n_outputdim], dtype=tf.float32, name='output_mask')\n",
    "    net = tflearn.lstm(net, n_hidden, return_seq=True, name=\"lstm_1\")\n",
    "    net = tflearn.lstm(net, n_outputdim, return_seq=True, name=\"lstm_2\")\n",
    "    net = tf.stack(net, axis=1)\n",
    "    preds = net\n",
    "    net = net * output_mask\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='mean_square')\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit([ input_data_, output_mask_], target_data_, validation_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'output_mask:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_1:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData_1/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_2:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData_2/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_3:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'InputData_3/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'input_data:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_4:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_1:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_5:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_2:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_6:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_3:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_7:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_4:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_8:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_5/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'input_data_6/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_10/X:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_7/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_11/X:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset_utils.load_data(filename=\"../synthetic_data/1000stud_100seq_expert.pickle\")\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_timesteps, n_inputdim = input_data_.shape\n",
    "_,_,n_outputdim = target_data_.shape\n",
    "print n_samples\n",
    "print n_timesteps\n",
    "print n_inputdim\n",
    "print n_outputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 2.358s\n",
      "| Adam | epoch: 032 | loss: 0.02480 -- iter: 896/900\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 3.469s\n",
      "| Adam | epoch: 032 | loss: 0.02479 | val_loss: 0.02476 -- iter: 900/900\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "with graph_to_use.as_default():\n",
    "    net = tflearn.input_data([None, n_timesteps, n_inputdim],dtype=tf.float32, name='input_data')\n",
    "    output_mask = tflearn.input_data([None, n_timesteps, n_outputdim], dtype=tf.float32, name='output_mask')\n",
    "    net = tflearn.lstm(net, n_hidden, return_seq=True, name=\"lstm_1\")\n",
    "    net = tflearn.lstm(net, n_outputdim, return_seq=True, name=\"lstm_2\")\n",
    "    net = tf.stack(net, axis=1)\n",
    "    preds = net\n",
    "    net = net * output_mask\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='mean_square')\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=2)\n",
    "    model.fit([ input_data_, output_mask_], target_data_, n_epoch=32, validation_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dynamics_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test_model/\n",
      "Checkpoint directory path: ../checkpoints/test_model/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_id=\"test_model\", load_checkpoint=False, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = (input_data_[:,:10,:], output_mask_[:,:10,:], target_data_[:,:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.03538\u001b[0m\u001b[0m | time: 0.681s\n",
      "| Adam | epoch: 003 | loss: 0.03538 -- iter: 896/900\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.03482\u001b[0m\u001b[0m | time: 1.726s\n",
      "| Adam | epoch: 003 | loss: 0.03482 | val_loss: 0.03224 -- iter: 900/900\n",
      "--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-175a1436c9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lisa1010/dev/smart-tutor/code/dynamics_model.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, load_checkpoint)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mdate_time_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m-%d-%Y_%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_time_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    213\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0;31m# Epoch end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                     \u001b[0mcaller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, training_state)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, training_state)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, training_step)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, model_file, global_step)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1373\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     return export_meta_graph(\n\u001b[1;32m   1402\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2187\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtoo\u001b[0m \u001b[0mlarge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \"\"\"\n\u001b[0;32m-> 2189\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2146\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m\"_output_shapes\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m             graph.node[-1].attr[\"_output_shapes\"].list.shape.extend([\n\u001b[0m\u001b[1;32m   2149\u001b[0m                 output.get_shape().as_proto() for output in op.outputs])\n\u001b[1;32m   2150\u001b[0m           \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0;31m# Construct a new object to represent this field.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       \u001b[0;31m# Atomically check if another thread has preempted us and, if not, swap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mMakeSubMessageDefault\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    424\u001b[0m       result._SetListener(\n\u001b[1;32m    425\u001b[0m           \u001b[0m_OneofListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontaining_oneof\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m           else message._listener_for_children)\n\u001b[1;32m    428\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent_message, field)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       \u001b[0mfield\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfield\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparent\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \"\"\"\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OneofListener\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.014513563364744186,\n",
       "   0.0470220223069191,\n",
       "   0.040145840495824814,\n",
       "   0.09161123633384705,\n",
       "   0.10280773788690567,\n",
       "   0.06935103237628937,\n",
       "   0.05226275697350502,\n",
       "   0.041719403117895126,\n",
       "   0.04879506677389145,\n",
       "   -0.07392476499080658],\n",
       "  [-0.03218340128660202,\n",
       "   0.07135201245546341,\n",
       "   0.0685245469212532,\n",
       "   0.1436130404472351,\n",
       "   0.19665291905403137,\n",
       "   0.1438368409872055,\n",
       "   0.1049000471830368,\n",
       "   0.06255870312452316,\n",
       "   0.1026017889380455,\n",
       "   -0.11450488120317459],\n",
       "  [-0.06921423971652985,\n",
       "   0.08478009700775146,\n",
       "   0.1043052151799202,\n",
       "   0.19068250060081482,\n",
       "   0.2767428159713745,\n",
       "   0.23720687627792358,\n",
       "   0.17555317282676697,\n",
       "   0.10750668495893478,\n",
       "   0.16081872582435608,\n",
       "   -0.14396199584007263],\n",
       "  [-0.11939936876296997,\n",
       "   0.10138699412345886,\n",
       "   0.13752755522727966,\n",
       "   0.24032481014728546,\n",
       "   0.3501714766025543,\n",
       "   0.32600459456443787,\n",
       "   0.26092734932899475,\n",
       "   0.16472099721431732,\n",
       "   0.22535595297813416,\n",
       "   -0.17225711047649384],\n",
       "  [-0.17461079359054565,\n",
       "   0.1152421087026596,\n",
       "   0.1707264631986618,\n",
       "   0.2937884032726288,\n",
       "   0.4026610255241394,\n",
       "   0.4047226011753082,\n",
       "   0.3470379114151001,\n",
       "   0.2304689586162567,\n",
       "   0.2942657470703125,\n",
       "   -0.20128174126148224],\n",
       "  [-0.23769868910312653,\n",
       "   0.1092974990606308,\n",
       "   0.2082236111164093,\n",
       "   0.3569919466972351,\n",
       "   0.4412563443183899,\n",
       "   0.4894976019859314,\n",
       "   0.422652930021286,\n",
       "   0.32053667306900024,\n",
       "   0.35850971937179565,\n",
       "   -0.20820315182209015],\n",
       "  [-0.2928321361541748,\n",
       "   0.11726514250040054,\n",
       "   0.22756768763065338,\n",
       "   0.41967979073524475,\n",
       "   0.4618801772594452,\n",
       "   0.5404962301254272,\n",
       "   0.47515639662742615,\n",
       "   0.4064711928367615,\n",
       "   0.41023027896881104,\n",
       "   -0.218476802110672],\n",
       "  [-0.34365835785865784,\n",
       "   0.11928804963827133,\n",
       "   0.2321077436208725,\n",
       "   0.4715743064880371,\n",
       "   0.4840993285179138,\n",
       "   0.5689555406570435,\n",
       "   0.5109775066375732,\n",
       "   0.4563242495059967,\n",
       "   0.46420493721961975,\n",
       "   -0.2140037715435028],\n",
       "  [-0.38566485047340393,\n",
       "   0.1281179040670395,\n",
       "   0.24280832707881927,\n",
       "   0.5128179788589478,\n",
       "   0.48435699939727783,\n",
       "   0.6077094674110413,\n",
       "   0.544141948223114,\n",
       "   0.49809062480926514,\n",
       "   0.5067498683929443,\n",
       "   -0.2280215620994568]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03127816319465637,\n",
       "   0.09107029438018799,\n",
       "   0.07287603616714478,\n",
       "   0.15671797096729279,\n",
       "   0.1430317461490631,\n",
       "   0.14184020459651947,\n",
       "   0.09511943906545639,\n",
       "   0.050354160368442535,\n",
       "   0.09149425476789474,\n",
       "   -0.11039866507053375],\n",
       "  [-0.060976751148700714,\n",
       "   0.09801477193832397,\n",
       "   0.11476847529411316,\n",
       "   0.24379442632198334,\n",
       "   0.2399071305990219,\n",
       "   0.21377906203269958,\n",
       "   0.16034163534641266,\n",
       "   0.07652704417705536,\n",
       "   0.14679443836212158,\n",
       "   -0.17103984951972961],\n",
       "  [-0.10324911773204803,\n",
       "   0.11362592875957489,\n",
       "   0.1525913029909134,\n",
       "   0.31725165247917175,\n",
       "   0.34183502197265625,\n",
       "   0.3059535622596741,\n",
       "   0.24026139080524445,\n",
       "   0.1149774044752121,\n",
       "   0.22379063069820404,\n",
       "   -0.2214277982711792],\n",
       "  [-0.1607619673013687,\n",
       "   0.12293191254138947,\n",
       "   0.1840815544128418,\n",
       "   0.38961559534072876,\n",
       "   0.40708380937576294,\n",
       "   0.4095291197299957,\n",
       "   0.3294735550880432,\n",
       "   0.18316788971424103,\n",
       "   0.2974473237991333,\n",
       "   -0.26686352491378784],\n",
       "  [-0.2251933366060257,\n",
       "   0.129822239279747,\n",
       "   0.21352751553058624,\n",
       "   0.45145541429519653,\n",
       "   0.44791433215141296,\n",
       "   0.5020326375961304,\n",
       "   0.4140762388706207,\n",
       "   0.2700747847557068,\n",
       "   0.3686193525791168,\n",
       "   -0.3029167652130127],\n",
       "  [-0.2910039722919464,\n",
       "   0.1354573369026184,\n",
       "   0.24534107744693756,\n",
       "   0.4955673813819885,\n",
       "   0.47051355242729187,\n",
       "   0.5658157467842102,\n",
       "   0.4853389859199524,\n",
       "   0.3722583055496216,\n",
       "   0.43880945444107056,\n",
       "   -0.3212301731109619],\n",
       "  [-0.3583813011646271,\n",
       "   0.1330350786447525,\n",
       "   0.279346227645874,\n",
       "   0.5353736877441406,\n",
       "   0.49029308557510376,\n",
       "   0.6222624182701111,\n",
       "   0.5359885692596436,\n",
       "   0.47413772344589233,\n",
       "   0.4909423291683197,\n",
       "   -0.30825361609458923],\n",
       "  [-0.4008702039718628,\n",
       "   0.14655952155590057,\n",
       "   0.29526978731155396,\n",
       "   0.5669235587120056,\n",
       "   0.4961823523044586,\n",
       "   0.6476258635520935,\n",
       "   0.5633453726768494,\n",
       "   0.547645092010498,\n",
       "   0.5222209692001343,\n",
       "   -0.30059829354286194]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.005862789694219828,\n",
       "   0.08617755770683289,\n",
       "   0.04532817006111145,\n",
       "   0.0842113122344017,\n",
       "   0.03539608418941498,\n",
       "   0.08861597627401352,\n",
       "   0.05721341073513031,\n",
       "   0.0165015310049057,\n",
       "   0.06334442645311356,\n",
       "   -0.03913375735282898],\n",
       "  [-0.031976114958524704,\n",
       "   0.1187528744339943,\n",
       "   0.07019568234682083,\n",
       "   0.15418589115142822,\n",
       "   0.10436864197254181,\n",
       "   0.15436528623104095,\n",
       "   0.09397280961275101,\n",
       "   0.04188372194766998,\n",
       "   0.0956481471657753,\n",
       "   -0.08248429745435715],\n",
       "  [-0.058634355664253235,\n",
       "   0.1337626725435257,\n",
       "   0.11179366707801819,\n",
       "   0.23167745769023895,\n",
       "   0.1990850567817688,\n",
       "   0.22867818176746368,\n",
       "   0.14849594235420227,\n",
       "   0.06368935108184814,\n",
       "   0.1434764862060547,\n",
       "   -0.15573088824748993],\n",
       "  [-0.09788542985916138,\n",
       "   0.14540064334869385,\n",
       "   0.1553751826286316,\n",
       "   0.30297383666038513,\n",
       "   0.3119584918022156,\n",
       "   0.3226514458656311,\n",
       "   0.22293721139431,\n",
       "   0.0977112278342247,\n",
       "   0.21700194478034973,\n",
       "   -0.21283042430877686],\n",
       "  [-0.1556023359298706,\n",
       "   0.14597055315971375,\n",
       "   0.19833093881607056,\n",
       "   0.36874639987945557,\n",
       "   0.38667625188827515,\n",
       "   0.4158157408237457,\n",
       "   0.3109106123447418,\n",
       "   0.16846641898155212,\n",
       "   0.2929220497608185,\n",
       "   -0.2547641396522522],\n",
       "  [-0.22068890929222107,\n",
       "   0.15480518341064453,\n",
       "   0.23436932265758514,\n",
       "   0.4227451980113983,\n",
       "   0.4372327923774719,\n",
       "   0.4868674576282501,\n",
       "   0.3958335220813751,\n",
       "   0.2547925114631653,\n",
       "   0.36704960465431213,\n",
       "   -0.29151248931884766],\n",
       "  [-0.29112106561660767,\n",
       "   0.1480877697467804,\n",
       "   0.27615466713905334,\n",
       "   0.4806373417377472,\n",
       "   0.46889474987983704,\n",
       "   0.5629793405532837,\n",
       "   0.47029396891593933,\n",
       "   0.36612704396247864,\n",
       "   0.436046302318573,\n",
       "   -0.2990073561668396],\n",
       "  [-0.3489680290222168,\n",
       "   0.15215113759040833,\n",
       "   0.29507625102996826,\n",
       "   0.5293710827827454,\n",
       "   0.47888946533203125,\n",
       "   0.6004736423492432,\n",
       "   0.5143836140632629,\n",
       "   0.4624844491481781,\n",
       "   0.4735715985298157,\n",
       "   -0.3080124258995056],\n",
       "  [-0.4017753005027771,\n",
       "   0.16196505725383759,\n",
       "   0.3105895519256592,\n",
       "   0.564799427986145,\n",
       "   0.4860957860946655,\n",
       "   0.6261220574378967,\n",
       "   0.5451611280441284,\n",
       "   0.5394534468650818,\n",
       "   0.5054194927215576,\n",
       "   -0.312355101108551]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03420348837971687,\n",
       "   0.08495858311653137,\n",
       "   0.0664568692445755,\n",
       "   0.15616393089294434,\n",
       "   0.13940481841564178,\n",
       "   0.1283305585384369,\n",
       "   0.08701346069574356,\n",
       "   0.05158423259854317,\n",
       "   0.08342994749546051,\n",
       "   -0.08944535255432129],\n",
       "  [-0.060907602310180664,\n",
       "   0.09095901250839233,\n",
       "   0.10269273072481155,\n",
       "   0.232721745967865,\n",
       "   0.23196962475776672,\n",
       "   0.18219909071922302,\n",
       "   0.13328084349632263,\n",
       "   0.0728285014629364,\n",
       "   0.12778891623020172,\n",
       "   -0.14081640541553497],\n",
       "  [-0.09361746907234192,\n",
       "   0.0981183871626854,\n",
       "   0.14935459196567535,\n",
       "   0.3159990906715393,\n",
       "   0.3259636163711548,\n",
       "   0.25963741540908813,\n",
       "   0.20035579800605774,\n",
       "   0.10048488527536392,\n",
       "   0.1935679167509079,\n",
       "   -0.22298680245876312],\n",
       "  [-0.1449628323316574,\n",
       "   0.09888078272342682,\n",
       "   0.19733908772468567,\n",
       "   0.40551644563674927,\n",
       "   0.4001621603965759,\n",
       "   0.3491721451282501,\n",
       "   0.2830325663089752,\n",
       "   0.14919288456439972,\n",
       "   0.2731110155582428,\n",
       "   -0.2958981990814209],\n",
       "  [-0.20487850904464722,\n",
       "   0.10200666636228561,\n",
       "   0.24171984195709229,\n",
       "   0.4854142367839813,\n",
       "   0.44695594906806946,\n",
       "   0.4349038898944855,\n",
       "   0.36876046657562256,\n",
       "   0.21863026916980743,\n",
       "   0.3542357385158539,\n",
       "   -0.36073917150497437],\n",
       "  [-0.2662322223186493,\n",
       "   0.12092211842536926,\n",
       "   0.28500714898109436,\n",
       "   0.5353193879127502,\n",
       "   0.4847032427787781,\n",
       "   0.5113058090209961,\n",
       "   0.44373786449432373,\n",
       "   0.3023030459880829,\n",
       "   0.43968331813812256,\n",
       "   -0.4079033434391022],\n",
       "  [-0.3269027769565582,\n",
       "   0.13777515292167664,\n",
       "   0.3185170888900757,\n",
       "   0.5725964307785034,\n",
       "   0.49580419063568115,\n",
       "   0.5755017995834351,\n",
       "   0.5088567733764648,\n",
       "   0.40885812044143677,\n",
       "   0.5052441358566284,\n",
       "   -0.4334279000759125],\n",
       "  [-0.3791249692440033,\n",
       "   0.16812054812908173,\n",
       "   0.3467302620410919,\n",
       "   0.5862396955490112,\n",
       "   0.5050480365753174,\n",
       "   0.6146808862686157,\n",
       "   0.546154797077179,\n",
       "   0.49721837043762207,\n",
       "   0.5499317646026611,\n",
       "   -0.4526500999927521]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.057247862219810486,\n",
       "   0.08008474111557007,\n",
       "   0.10546114295721054,\n",
       "   0.2235705405473709,\n",
       "   0.2839057445526123,\n",
       "   0.19722367823123932,\n",
       "   0.15593916177749634,\n",
       "   0.0836041271686554,\n",
       "   0.15329524874687195,\n",
       "   -0.1659594476222992],\n",
       "  [-0.10434599965810776,\n",
       "   0.09520581364631653,\n",
       "   0.1370006948709488,\n",
       "   0.29061993956565857,\n",
       "   0.3641490042209625,\n",
       "   0.3098030984401703,\n",
       "   0.24149750173091888,\n",
       "   0.13650289177894592,\n",
       "   0.22524765133857727,\n",
       "   -0.21232809126377106],\n",
       "  [-0.167626291513443,\n",
       "   0.10245420038700104,\n",
       "   0.17349250614643097,\n",
       "   0.34856370091438293,\n",
       "   0.41738319396972656,\n",
       "   0.4118785858154297,\n",
       "   0.3323492109775543,\n",
       "   0.21695390343666077,\n",
       "   0.30281487107276917,\n",
       "   -0.2412136197090149],\n",
       "  [-0.24291224777698517,\n",
       "   0.10032673925161362,\n",
       "   0.208566814661026,\n",
       "   0.40867307782173157,\n",
       "   0.4561443626880646,\n",
       "   0.5027825236320496,\n",
       "   0.41346457600593567,\n",
       "   0.32292020320892334,\n",
       "   0.37091270089149475,\n",
       "   -0.24030572175979614],\n",
       "  [-0.3041929602622986,\n",
       "   0.11394774913787842,\n",
       "   0.2258629947900772,\n",
       "   0.46385255455970764,\n",
       "   0.47372329235076904,\n",
       "   0.5544580817222595,\n",
       "   0.4730626344680786,\n",
       "   0.4212020933628082,\n",
       "   0.42455315589904785,\n",
       "   -0.24243329465389252],\n",
       "  [-0.3572836220264435,\n",
       "   0.1321299821138382,\n",
       "   0.22877410054206848,\n",
       "   0.5150938034057617,\n",
       "   0.489890992641449,\n",
       "   0.5821978449821472,\n",
       "   0.508150041103363,\n",
       "   0.47752436995506287,\n",
       "   0.4684116542339325,\n",
       "   -0.25712013244628906],\n",
       "  [-0.40285348892211914,\n",
       "   0.1360579878091812,\n",
       "   0.2240927666425705,\n",
       "   0.5503410696983337,\n",
       "   0.507268488407135,\n",
       "   0.602586567401886,\n",
       "   0.5363503098487854,\n",
       "   0.5220065116882324,\n",
       "   0.5163953304290771,\n",
       "   -0.25548022985458374]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.005862789694219828,\n",
       "   0.08617755770683289,\n",
       "   0.04532817006111145,\n",
       "   0.0842113122344017,\n",
       "   0.03539608418941498,\n",
       "   0.08861597627401352,\n",
       "   0.05721341073513031,\n",
       "   0.0165015310049057,\n",
       "   0.06334442645311356,\n",
       "   -0.03913375735282898],\n",
       "  [-0.023123623803257942,\n",
       "   0.13784949481487274,\n",
       "   0.07789395749568939,\n",
       "   0.1472705751657486,\n",
       "   0.07509306073188782,\n",
       "   0.1604142040014267,\n",
       "   0.09974164515733719,\n",
       "   0.02444310672581196,\n",
       "   0.10856691002845764,\n",
       "   -0.07207928597927094],\n",
       "  [-0.05961983650922775,\n",
       "   0.16516649723052979,\n",
       "   0.10900723189115524,\n",
       "   0.22902508080005646,\n",
       "   0.1622152030467987,\n",
       "   0.2404518872499466,\n",
       "   0.14683611690998077,\n",
       "   0.05489376187324524,\n",
       "   0.1488243192434311,\n",
       "   -0.12612617015838623],\n",
       "  [-0.09663838893175125,\n",
       "   0.17179463803768158,\n",
       "   0.15472890436649323,\n",
       "   0.31104403734207153,\n",
       "   0.2644428312778473,\n",
       "   0.32147520780563354,\n",
       "   0.21147173643112183,\n",
       "   0.08512993156909943,\n",
       "   0.20477068424224854,\n",
       "   -0.20753706991672516],\n",
       "  [-0.1476699709892273,\n",
       "   0.16157129406929016,\n",
       "   0.2033478319644928,\n",
       "   0.4017667770385742,\n",
       "   0.3551658093929291,\n",
       "   0.40257886052131653,\n",
       "   0.2912360429763794,\n",
       "   0.13407102227210999,\n",
       "   0.2749353349208832,\n",
       "   -0.2794758379459381],\n",
       "  [-0.20422206819057465,\n",
       "   0.16578540205955505,\n",
       "   0.25083914399147034,\n",
       "   0.46711477637290955,\n",
       "   0.43075132369995117,\n",
       "   0.47971758246421814,\n",
       "   0.3764747381210327,\n",
       "   0.2016037255525589,\n",
       "   0.36287787556648254,\n",
       "   -0.33587899804115295],\n",
       "  [-0.26813051104545593,\n",
       "   0.1662045270204544,\n",
       "   0.28966930508613586,\n",
       "   0.5192147493362427,\n",
       "   0.46377167105674744,\n",
       "   0.5471265316009521,\n",
       "   0.45559510588645935,\n",
       "   0.3056398928165436,\n",
       "   0.44026315212249756,\n",
       "   -0.37334367632865906],\n",
       "  [-0.3387893736362457,\n",
       "   0.16086536645889282,\n",
       "   0.3295435309410095,\n",
       "   0.5598940849304199,\n",
       "   0.4894082844257355,\n",
       "   0.6116238236427307,\n",
       "   0.5161793231964111,\n",
       "   0.4252799153327942,\n",
       "   0.5001445412635803,\n",
       "   -0.37833917140960693],\n",
       "  [-0.38768890500068665,\n",
       "   0.1690857857465744,\n",
       "   0.351093590259552,\n",
       "   0.5892294645309448,\n",
       "   0.49744564294815063,\n",
       "   0.6454594731330872,\n",
       "   0.5497063398361206,\n",
       "   0.5187475085258484,\n",
       "   0.5375459790229797,\n",
       "   -0.3840266764163971]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.058248504996299744,\n",
       "   0.0690665915608406,\n",
       "   0.11151129007339478,\n",
       "   0.2467028647661209,\n",
       "   0.2727585434913635,\n",
       "   0.17967411875724792,\n",
       "   0.15757513046264648,\n",
       "   0.08372584730386734,\n",
       "   0.1453004628419876,\n",
       "   -0.1782950609922409],\n",
       "  [-0.10167629271745682,\n",
       "   0.08879223465919495,\n",
       "   0.14719465374946594,\n",
       "   0.31988584995269775,\n",
       "   0.3662298917770386,\n",
       "   0.2775735855102539,\n",
       "   0.23900474607944489,\n",
       "   0.12316052615642548,\n",
       "   0.22674421966075897,\n",
       "   -0.22890068590641022],\n",
       "  [-0.1624550074338913,\n",
       "   0.09984000772237778,\n",
       "   0.18285717070102692,\n",
       "   0.3849408030509949,\n",
       "   0.42069122195243835,\n",
       "   0.37895315885543823,\n",
       "   0.3279584050178528,\n",
       "   0.19788342714309692,\n",
       "   0.3057538866996765,\n",
       "   -0.26531723141670227],\n",
       "  [-0.23845139145851135,\n",
       "   0.10098498314619064,\n",
       "   0.2175826132297516,\n",
       "   0.4453183114528656,\n",
       "   0.4605388939380646,\n",
       "   0.47577205300331116,\n",
       "   0.40886789560317993,\n",
       "   0.3037096858024597,\n",
       "   0.3751377761363983,\n",
       "   -0.27267539501190186],\n",
       "  [-0.30577918887138367,\n",
       "   0.11194641143083572,\n",
       "   0.23216237127780914,\n",
       "   0.4968763589859009,\n",
       "   0.474517285823822,\n",
       "   0.530599057674408,\n",
       "   0.4657686650753021,\n",
       "   0.4047018885612488,\n",
       "   0.42059555649757385,\n",
       "   -0.27866390347480774],\n",
       "  [-0.3670070171356201,\n",
       "   0.12681278586387634,\n",
       "   0.24325476586818695,\n",
       "   0.5356999039649963,\n",
       "   0.48277804255485535,\n",
       "   0.5690165162086487,\n",
       "   0.507861852645874,\n",
       "   0.4898432195186615,\n",
       "   0.45843878388404846,\n",
       "   -0.2800208032131195],\n",
       "  [-0.4167822003364563,\n",
       "   0.14109325408935547,\n",
       "   0.250868558883667,\n",
       "   0.5658891201019287,\n",
       "   0.4877112805843353,\n",
       "   0.5969432592391968,\n",
       "   0.5366802215576172,\n",
       "   0.5557106733322144,\n",
       "   0.4856134355068207,\n",
       "   -0.2771853506565094]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.058248504996299744,\n",
       "   0.0690665915608406,\n",
       "   0.11151129007339478,\n",
       "   0.2467028647661209,\n",
       "   0.2727585434913635,\n",
       "   0.17967411875724792,\n",
       "   0.15757513046264648,\n",
       "   0.08372584730386734,\n",
       "   0.1453004628419876,\n",
       "   -0.1782950609922409],\n",
       "  [-0.10211649537086487,\n",
       "   0.07676362991333008,\n",
       "   0.15105488896369934,\n",
       "   0.34038570523262024,\n",
       "   0.3536534905433655,\n",
       "   0.26201990246772766,\n",
       "   0.23862825334072113,\n",
       "   0.12332990765571594,\n",
       "   0.21722912788391113,\n",
       "   -0.23994718492031097],\n",
       "  [-0.1574753373861313,\n",
       "   0.0961245521903038,\n",
       "   0.18575678765773773,\n",
       "   0.412643164396286,\n",
       "   0.42478057742118835,\n",
       "   0.35893985629081726,\n",
       "   0.32350555062294006,\n",
       "   0.17743146419525146,\n",
       "   0.3039356470108032,\n",
       "   -0.2907536029815674],\n",
       "  [-0.22027722001075745,\n",
       "   0.11041035503149033,\n",
       "   0.2142597734928131,\n",
       "   0.4794926047325134,\n",
       "   0.45950832962989807,\n",
       "   0.46049273014068604,\n",
       "   0.40720677375793457,\n",
       "   0.25972336530685425,\n",
       "   0.3760809004306793,\n",
       "   -0.3337879776954651],\n",
       "  [-0.2838820815086365,\n",
       "   0.12443418055772781,\n",
       "   0.24316547811031342,\n",
       "   0.528627872467041,\n",
       "   0.48025697469711304,\n",
       "   0.5452204942703247,\n",
       "   0.47800135612487793,\n",
       "   0.35232090950012207,\n",
       "   0.43860000371932983,\n",
       "   -0.3651861548423767],\n",
       "  [-0.34057822823524475,\n",
       "   0.13886594772338867,\n",
       "   0.27253884077072144,\n",
       "   0.5640837550163269,\n",
       "   0.4918398857116699,\n",
       "   0.607871949672699,\n",
       "   0.5323774814605713,\n",
       "   0.4408590495586395,\n",
       "   0.48914486169815063,\n",
       "   -0.3854181170463562],\n",
       "  [-0.38752374053001404,\n",
       "   0.15441176295280457,\n",
       "   0.3025898039340973,\n",
       "   0.5882860422134399,\n",
       "   0.49890464544296265,\n",
       "   0.6517040729522705,\n",
       "   0.5718855857849121,\n",
       "   0.5152390003204346,\n",
       "   0.5280877351760864,\n",
       "   -0.3967254161834717]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03127816319465637,\n",
       "   0.09107029438018799,\n",
       "   0.07287603616714478,\n",
       "   0.15671797096729279,\n",
       "   0.1430317461490631,\n",
       "   0.14184020459651947,\n",
       "   0.09511943906545639,\n",
       "   0.050354160368442535,\n",
       "   0.09149425476789474,\n",
       "   -0.11039866507053375],\n",
       "  [-0.059480901807546616,\n",
       "   0.11047469079494476,\n",
       "   0.10961417853832245,\n",
       "   0.22083410620689392,\n",
       "   0.2510247528553009,\n",
       "   0.23019792139530182,\n",
       "   0.15908633172512054,\n",
       "   0.07584737241268158,\n",
       "   0.15513871610164642,\n",
       "   -0.16032931208610535],\n",
       "  [-0.10637495666742325,\n",
       "   0.1215302050113678,\n",
       "   0.14279113709926605,\n",
       "   0.28813230991363525,\n",
       "   0.3402049243450165,\n",
       "   0.3377043902873993,\n",
       "   0.2429751753807068,\n",
       "   0.12786360085010529,\n",
       "   0.22257700562477112,\n",
       "   -0.2063126415014267],\n",
       "  [-0.16847631335258484,\n",
       "   0.12283267825841904,\n",
       "   0.18033909797668457,\n",
       "   0.34652021527290344,\n",
       "   0.40150222182273865,\n",
       "   0.43177521228790283,\n",
       "   0.33234745264053345,\n",
       "   0.2077411562204361,\n",
       "   0.29760900139808655,\n",
       "   -0.23532544076442719],\n",
       "  [-0.23343591392040253,\n",
       "   0.13489453494548798,\n",
       "   0.21264009177684784,\n",
       "   0.39830711483955383,\n",
       "   0.4422001242637634,\n",
       "   0.4987843334674835,\n",
       "   0.41326168179512024,\n",
       "   0.2947700619697571,\n",
       "   0.3689412474632263,\n",
       "   -0.26142585277557373],\n",
       "  [-0.3018285632133484,\n",
       "   0.1307247281074524,\n",
       "   0.2514607012271881,\n",
       "   0.4561883807182312,\n",
       "   0.4691040813922882,\n",
       "   0.5695794224739075,\n",
       "   0.48165783286094666,\n",
       "   0.3989560306072235,\n",
       "   0.43338218331336975,\n",
       "   -0.2604142129421234],\n",
       "  [-0.3521101176738739,\n",
       "   0.141281396150589,\n",
       "   0.271355003118515,\n",
       "   0.505827009677887,\n",
       "   0.48101359605789185,\n",
       "   0.6069961786270142,\n",
       "   0.5242564678192139,\n",
       "   0.48353955149650574,\n",
       "   0.476498544216156,\n",
       "   -0.26560845971107483],\n",
       "  [-0.39889639616012573,\n",
       "   0.14655019342899323,\n",
       "   0.2772831618785858,\n",
       "   0.5432493686676025,\n",
       "   0.5000923871994019,\n",
       "   0.6239126920700073,\n",
       "   0.5520086288452148,\n",
       "   0.5288835763931274,\n",
       "   0.5202540159225464,\n",
       "   -0.25520721077919006]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.057247862219810486,\n",
       "   0.08008474111557007,\n",
       "   0.10546114295721054,\n",
       "   0.2235705405473709,\n",
       "   0.2839057445526123,\n",
       "   0.19722367823123932,\n",
       "   0.15593916177749634,\n",
       "   0.0836041271686554,\n",
       "   0.15329524874687195,\n",
       "   -0.1659594476222992],\n",
       "  [-0.10634910315275192,\n",
       "   0.09204169362783432,\n",
       "   0.14448414742946625,\n",
       "   0.2828052043914795,\n",
       "   0.3588876724243164,\n",
       "   0.30093318223953247,\n",
       "   0.23928526043891907,\n",
       "   0.14087560772895813,\n",
       "   0.22557410597801208,\n",
       "   -0.20268292725086212],\n",
       "  [-0.16713985800743103,\n",
       "   0.10960502922534943,\n",
       "   0.17833401262760162,\n",
       "   0.3384683132171631,\n",
       "   0.415674090385437,\n",
       "   0.39195284247398376,\n",
       "   0.3292734920978546,\n",
       "   0.2125479131937027,\n",
       "   0.299818217754364,\n",
       "   -0.23643545806407928],\n",
       "  [-0.23756016790866852,\n",
       "   0.10867643356323242,\n",
       "   0.2169070541858673,\n",
       "   0.4016650319099426,\n",
       "   0.45342424511909485,\n",
       "   0.4848024845123291,\n",
       "   0.4118563234806061,\n",
       "   0.31293436884880066,\n",
       "   0.3700428307056427,\n",
       "   -0.24362049996852875],\n",
       "  [-0.3006177842617035,\n",
       "   0.11638326942920685,\n",
       "   0.2345806509256363,\n",
       "   0.4584605097770691,\n",
       "   0.4678575098514557,\n",
       "   0.536447286605835,\n",
       "   0.468919575214386,\n",
       "   0.4064263701438904,\n",
       "   0.4150925278663635,\n",
       "   -0.25325480103492737],\n",
       "  [-0.3589994013309479,\n",
       "   0.12805280089378357,\n",
       "   0.2480306476354599,\n",
       "   0.5039001703262329,\n",
       "   0.47728273272514343,\n",
       "   0.5728580355644226,\n",
       "   0.5102958679199219,\n",
       "   0.48629629611968994,\n",
       "   0.4536206126213074,\n",
       "   -0.2593274414539337],\n",
       "  [-0.40764355659484863,\n",
       "   0.13997741043567657,\n",
       "   0.2571384608745575,\n",
       "   0.54050213098526,\n",
       "   0.4833797812461853,\n",
       "   0.5996587872505188,\n",
       "   0.5384678840637207,\n",
       "   0.5493736267089844,\n",
       "   0.4821021854877472,\n",
       "   -0.2614176869392395]]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, input_data_[:10, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynamics_model_class as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test_model/\n",
      "Checkpoint directory path: ../checkpoints/test_model/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "dmodel = dm.DynamicsModel(model_id=\"test_model\",  load_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.567s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 068 | loss: 0.02487 -- iter: 896/900\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-56596559dd51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lisa1010/dev/smart-tutor/code/dynamics_model_class.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, load_checkpoint)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdate_time_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m-%d-%Y_%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_time_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    213\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    331\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow_metric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                 \u001b[0meval_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow_metric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mevaluate_flow\u001b[0;34m(session, ops_to_evaluate, dataflow)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0mfeed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/data_flow.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/Queue.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dmodel.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.49599671363830566,\n",
       "   0.505977988243103,\n",
       "   0.49741360545158386,\n",
       "   0.5000333189964294,\n",
       "   0.498515248298645,\n",
       "   0.5003623366355896,\n",
       "   0.5020152926445007,\n",
       "   0.4968193471431732,\n",
       "   0.5021724104881287,\n",
       "   0.49825751781463623],\n",
       "  [0.48964592814445496,\n",
       "   0.5083518624305725,\n",
       "   0.49532902240753174,\n",
       "   0.5052512884140015,\n",
       "   0.49550819396972656,\n",
       "   0.5047191381454468,\n",
       "   0.49943220615386963,\n",
       "   0.49179941415786743,\n",
       "   0.5016341805458069,\n",
       "   0.4988301992416382],\n",
       "  [0.48247480392456055,\n",
       "   0.5100704431533813,\n",
       "   0.4926689863204956,\n",
       "   0.5104095339775085,\n",
       "   0.4944813549518585,\n",
       "   0.5111148357391357,\n",
       "   0.49826157093048096,\n",
       "   0.4853592813014984,\n",
       "   0.4960009455680847,\n",
       "   0.49970728158950806],\n",
       "  [0.4703198969364166,\n",
       "   0.5135535001754761,\n",
       "   0.4915870130062103,\n",
       "   0.5127080678939819,\n",
       "   0.49413204193115234,\n",
       "   0.5221576690673828,\n",
       "   0.502672016620636,\n",
       "   0.47870734333992004,\n",
       "   0.4900960922241211,\n",
       "   0.5019947290420532],\n",
       "  [0.4537630081176758,\n",
       "   0.520048201084137,\n",
       "   0.49032503366470337,\n",
       "   0.5139161944389343,\n",
       "   0.4934532642364502,\n",
       "   0.5316247344017029,\n",
       "   0.5167728662490845,\n",
       "   0.4716108441352844,\n",
       "   0.4958451986312866,\n",
       "   0.4966897964477539],\n",
       "  [0.43617871403694153,\n",
       "   0.5289715528488159,\n",
       "   0.48939430713653564,\n",
       "   0.5126253962516785,\n",
       "   0.4925546646118164,\n",
       "   0.5344418883323669,\n",
       "   0.5401607155799866,\n",
       "   0.4698314368724823,\n",
       "   0.5158414244651794,\n",
       "   0.4840362071990967],\n",
       "  [0.42516717314720154,\n",
       "   0.5388312339782715,\n",
       "   0.48898425698280334,\n",
       "   0.508516788482666,\n",
       "   0.49308663606643677,\n",
       "   0.5259639620780945,\n",
       "   0.5653541088104248,\n",
       "   0.4776161313056946,\n",
       "   0.5385245084762573,\n",
       "   0.46874111890792847],\n",
       "  [0.4183195233345032,\n",
       "   0.5485021471977234,\n",
       "   0.4903010129928589,\n",
       "   0.5027713179588318,\n",
       "   0.48928093910217285,\n",
       "   0.5091527700424194,\n",
       "   0.5853952765464783,\n",
       "   0.4925719201564789,\n",
       "   0.557790994644165,\n",
       "   0.456940621137619],\n",
       "  [0.41411229968070984,\n",
       "   0.559482753276825,\n",
       "   0.49123120307922363,\n",
       "   0.497923344373703,\n",
       "   0.4830032289028168,\n",
       "   0.48612239956855774,\n",
       "   0.6010857820510864,\n",
       "   0.5239127278327942,\n",
       "   0.5803111791610718,\n",
       "   0.44925323128700256],\n",
       "  [0.4249594509601593,\n",
       "   0.569526195526123,\n",
       "   0.4904220998287201,\n",
       "   0.4949359595775604,\n",
       "   0.46843722462654114,\n",
       "   0.45716696977615356,\n",
       "   0.6109771728515625,\n",
       "   0.5648123025894165,\n",
       "   0.5905900597572327,\n",
       "   0.4446971118450165]],\n",
       " [[0.4935073256492615,\n",
       "   0.5066367387771606,\n",
       "   0.5006406307220459,\n",
       "   0.4931355118751526,\n",
       "   0.5011196732521057,\n",
       "   0.5027900338172913,\n",
       "   0.5059390664100647,\n",
       "   0.5037103891372681,\n",
       "   0.5045592188835144,\n",
       "   0.49498361349105835],\n",
       "  [0.4888042211532593,\n",
       "   0.5124666690826416,\n",
       "   0.49886104464530945,\n",
       "   0.48768845200538635,\n",
       "   0.502082884311676,\n",
       "   0.5015733242034912,\n",
       "   0.5139681100845337,\n",
       "   0.5072578191757202,\n",
       "   0.5110902190208435,\n",
       "   0.4893607795238495],\n",
       "  [0.4837706387042999,\n",
       "   0.5132308006286621,\n",
       "   0.4966180622577667,\n",
       "   0.4896356761455536,\n",
       "   0.5010759830474854,\n",
       "   0.5032262802124023,\n",
       "   0.5173490047454834,\n",
       "   0.5093849897384644,\n",
       "   0.5153816342353821,\n",
       "   0.48739975690841675],\n",
       "  [0.47697913646698,\n",
       "   0.5159948468208313,\n",
       "   0.4962058365345001,\n",
       "   0.5016492605209351,\n",
       "   0.5036230087280273,\n",
       "   0.5028076767921448,\n",
       "   0.5183938145637512,\n",
       "   0.5096410512924194,\n",
       "   0.5118613839149475,\n",
       "   0.48518186807632446],\n",
       "  [0.47057032585144043,\n",
       "   0.5163217782974243,\n",
       "   0.49360454082489014,\n",
       "   0.5125990509986877,\n",
       "   0.5019816160202026,\n",
       "   0.5014887452125549,\n",
       "   0.5191658139228821,\n",
       "   0.5111626386642456,\n",
       "   0.5062094330787659,\n",
       "   0.483379989862442],\n",
       "  [0.4688839614391327,\n",
       "   0.5220568776130676,\n",
       "   0.49056243896484375,\n",
       "   0.5243667960166931,\n",
       "   0.49032846093177795,\n",
       "   0.4946741461753845,\n",
       "   0.5149816870689392,\n",
       "   0.5242074728012085,\n",
       "   0.5099191665649414,\n",
       "   0.47194865345954895],\n",
       "  [0.47019073367118835,\n",
       "   0.5310572981834412,\n",
       "   0.4862220287322998,\n",
       "   0.5377153158187866,\n",
       "   0.47338351607322693,\n",
       "   0.4833642244338989,\n",
       "   0.4996504783630371,\n",
       "   0.5456037521362305,\n",
       "   0.5182302594184875,\n",
       "   0.456155389547348],\n",
       "  [0.469103068113327,\n",
       "   0.5361096262931824,\n",
       "   0.47985541820526123,\n",
       "   0.5500144362449646,\n",
       "   0.4661874771118164,\n",
       "   0.475056529045105,\n",
       "   0.4756297767162323,\n",
       "   0.5621740221977234,\n",
       "   0.5216403603553772,\n",
       "   0.4482935965061188],\n",
       "  [0.4695298969745636,\n",
       "   0.5345526933670044,\n",
       "   0.4739120900630951,\n",
       "   0.5572773814201355,\n",
       "   0.4666861593723297,\n",
       "   0.4686569571495056,\n",
       "   0.4480878710746765,\n",
       "   0.5711529850959778,\n",
       "   0.522301435470581,\n",
       "   0.44190213084220886],\n",
       "  [0.468523770570755,\n",
       "   0.525304913520813,\n",
       "   0.4741703271865845,\n",
       "   0.5627244710922241,\n",
       "   0.4680138826370239,\n",
       "   0.4681204557418823,\n",
       "   0.42252233624458313,\n",
       "   0.5644826889038086,\n",
       "   0.5175504088401794,\n",
       "   0.4397382140159607]],\n",
       " [[0.4935073256492615,\n",
       "   0.5066367387771606,\n",
       "   0.5006406307220459,\n",
       "   0.4931355118751526,\n",
       "   0.5011196732521057,\n",
       "   0.5027900338172913,\n",
       "   0.5059390664100647,\n",
       "   0.5037103891372681,\n",
       "   0.5045592188835144,\n",
       "   0.49498361349105835],\n",
       "  [0.4864151179790497,\n",
       "   0.5134059190750122,\n",
       "   0.5013522505760193,\n",
       "   0.4798593819141388,\n",
       "   0.5054095983505249,\n",
       "   0.503986120223999,\n",
       "   0.5183268785476685,\n",
       "   0.5136097073554993,\n",
       "   0.5136396884918213,\n",
       "   0.4860149323940277],\n",
       "  [0.48251864314079285,\n",
       "   0.5177899599075317,\n",
       "   0.49937862157821655,\n",
       "   0.4698704779148102,\n",
       "   0.5096774697303772,\n",
       "   0.4998442530632019,\n",
       "   0.5328044295310974,\n",
       "   0.5246942043304443,\n",
       "   0.5241942405700684,\n",
       "   0.4782741963863373],\n",
       "  [0.48027995228767395,\n",
       "   0.5149013996124268,\n",
       "   0.49656760692596436,\n",
       "   0.46929213404655457,\n",
       "   0.511268675327301,\n",
       "   0.4979315996170044,\n",
       "   0.5424811840057373,\n",
       "   0.5347463488578796,\n",
       "   0.5325235724449158,\n",
       "   0.4752316176891327],\n",
       "  [0.48054632544517517,\n",
       "   0.5101013779640198,\n",
       "   0.49272894859313965,\n",
       "   0.4788307845592499,\n",
       "   0.5128154158592224,\n",
       "   0.495523601770401,\n",
       "   0.5500748753547668,\n",
       "   0.5437270998954773,\n",
       "   0.5377021431922913,\n",
       "   0.473631352186203],\n",
       "  [0.47937169671058655,\n",
       "   0.5055421590805054,\n",
       "   0.48871538043022156,\n",
       "   0.49055591225624084,\n",
       "   0.5119155049324036,\n",
       "   0.497022807598114,\n",
       "   0.5583524703979492,\n",
       "   0.5503260493278503,\n",
       "   0.5420855283737183,\n",
       "   0.47418126463890076],\n",
       "  [0.47849005460739136,\n",
       "   0.5042028427124023,\n",
       "   0.48473697900772095,\n",
       "   0.5034549832344055,\n",
       "   0.5114812254905701,\n",
       "   0.4919143319129944,\n",
       "   0.5679455995559692,\n",
       "   0.5568810105323792,\n",
       "   0.552671492099762,\n",
       "   0.4691372215747833],\n",
       "  [0.47729960083961487,\n",
       "   0.5018625855445862,\n",
       "   0.4813368618488312,\n",
       "   0.5148546695709229,\n",
       "   0.5120936632156372,\n",
       "   0.482450932264328,\n",
       "   0.5753395557403564,\n",
       "   0.5665517449378967,\n",
       "   0.5611532926559448,\n",
       "   0.4625680148601532],\n",
       "  [0.4716723561286926,\n",
       "   0.4921557605266571,\n",
       "   0.4792180359363556,\n",
       "   0.5260759592056274,\n",
       "   0.5021523833274841,\n",
       "   0.4689690172672272,\n",
       "   0.5777547359466553,\n",
       "   0.5858073830604553,\n",
       "   0.5734472870826721,\n",
       "   0.4575400948524475],\n",
       "  [0.46615177392959595,\n",
       "   0.47498705983161926,\n",
       "   0.47839879989624023,\n",
       "   0.5374900102615356,\n",
       "   0.48372694849967957,\n",
       "   0.4545191824436188,\n",
       "   0.5819700360298157,\n",
       "   0.609631359577179,\n",
       "   0.585426926612854,\n",
       "   0.4527510702610016]],\n",
       " [[0.4935073256492615,\n",
       "   0.5066367387771606,\n",
       "   0.5006406307220459,\n",
       "   0.4931355118751526,\n",
       "   0.5011196732521057,\n",
       "   0.5027900338172913,\n",
       "   0.5059390664100647,\n",
       "   0.5037103891372681,\n",
       "   0.5045592188835144,\n",
       "   0.49498361349105835],\n",
       "  [0.4888042211532593,\n",
       "   0.5124666690826416,\n",
       "   0.49886104464530945,\n",
       "   0.48768845200538635,\n",
       "   0.502082884311676,\n",
       "   0.5015733242034912,\n",
       "   0.5139681100845337,\n",
       "   0.5072578191757202,\n",
       "   0.5110902190208435,\n",
       "   0.4893607795238495],\n",
       "  [0.4929240345954895,\n",
       "   0.516448974609375,\n",
       "   0.4938470125198364,\n",
       "   0.49369972944259644,\n",
       "   0.5044259428977966,\n",
       "   0.4983064830303192,\n",
       "   0.5112334489822388,\n",
       "   0.5068015456199646,\n",
       "   0.5119608640670776,\n",
       "   0.4841019809246063],\n",
       "  [0.5007560849189758,\n",
       "   0.5182065367698669,\n",
       "   0.4872228801250458,\n",
       "   0.5079019665718079,\n",
       "   0.5078688263893127,\n",
       "   0.49604564905166626,\n",
       "   0.4969274699687958,\n",
       "   0.5014709830284119,\n",
       "   0.5072579383850098,\n",
       "   0.48117199540138245],\n",
       "  [0.5020344257354736,\n",
       "   0.5155802965164185,\n",
       "   0.48553648591041565,\n",
       "   0.521838903427124,\n",
       "   0.5078266859054565,\n",
       "   0.5006723403930664,\n",
       "   0.4766537845134735,\n",
       "   0.49318230152130127,\n",
       "   0.4999012053012848,\n",
       "   0.4852582514286041],\n",
       "  [0.4984493851661682,\n",
       "   0.5138178467750549,\n",
       "   0.4931369125843048,\n",
       "   0.5304259061813354,\n",
       "   0.5104289650917053,\n",
       "   0.5083906054496765,\n",
       "   0.4594757854938507,\n",
       "   0.48229333758354187,\n",
       "   0.483430951833725,\n",
       "   0.49318283796310425],\n",
       "  [0.48839783668518066,\n",
       "   0.5124683380126953,\n",
       "   0.5079970359802246,\n",
       "   0.5351042151451111,\n",
       "   0.5117958188056946,\n",
       "   0.5207926630973816,\n",
       "   0.4490090012550354,\n",
       "   0.4710412621498108,\n",
       "   0.4594714045524597,\n",
       "   0.5025239586830139],\n",
       "  [0.4691604673862457,\n",
       "   0.5104724168777466,\n",
       "   0.5210692286491394,\n",
       "   0.5382067561149597,\n",
       "   0.5093942284584045,\n",
       "   0.5367593765258789,\n",
       "   0.44400638341903687,\n",
       "   0.4590891897678375,\n",
       "   0.4302017092704773,\n",
       "   0.513044536113739],\n",
       "  [0.43615126609802246,\n",
       "   0.5113045573234558,\n",
       "   0.5329358577728271,\n",
       "   0.5375150442123413,\n",
       "   0.5057876706123352,\n",
       "   0.558469295501709,\n",
       "   0.44797810912132263,\n",
       "   0.44826582074165344,\n",
       "   0.4029180407524109,\n",
       "   0.5220268368721008],\n",
       "  [0.39566367864608765,\n",
       "   0.5144246220588684,\n",
       "   0.5379319787025452,\n",
       "   0.5362714529037476,\n",
       "   0.5033884644508362,\n",
       "   0.5835130214691162,\n",
       "   0.4657069146633148,\n",
       "   0.4361811578273773,\n",
       "   0.38456860184669495,\n",
       "   0.5213557481765747]],\n",
       " [[0.49599671363830566,\n",
       "   0.505977988243103,\n",
       "   0.49741360545158386,\n",
       "   0.5000333189964294,\n",
       "   0.498515248298645,\n",
       "   0.5003623366355896,\n",
       "   0.5020152926445007,\n",
       "   0.4968193471431732,\n",
       "   0.5021724104881287,\n",
       "   0.49825751781463623],\n",
       "  [0.4982739984989166,\n",
       "   0.5114309191703796,\n",
       "   0.4927102029323578,\n",
       "   0.5087923407554626,\n",
       "   0.4982874393463135,\n",
       "   0.5002113580703735,\n",
       "   0.4950408637523651,\n",
       "   0.49080711603164673,\n",
       "   0.4985194206237793,\n",
       "   0.495974600315094],\n",
       "  [0.495180606842041,\n",
       "   0.5128109455108643,\n",
       "   0.49134019017219543,\n",
       "   0.5186378359794617,\n",
       "   0.4964984357357025,\n",
       "   0.505379319190979,\n",
       "   0.4830396771430969,\n",
       "   0.4827333390712738,\n",
       "   0.4921058714389801,\n",
       "   0.49838364124298096],\n",
       "  [0.488839328289032,\n",
       "   0.5130492448806763,\n",
       "   0.4918130338191986,\n",
       "   0.5242824554443359,\n",
       "   0.4961698055267334,\n",
       "   0.5140914916992188,\n",
       "   0.47307127714157104,\n",
       "   0.47317343950271606,\n",
       "   0.4796505272388458,\n",
       "   0.5027008652687073],\n",
       "  [0.48134538531303406,\n",
       "   0.5185043215751648,\n",
       "   0.4974914789199829,\n",
       "   0.5281264185905457,\n",
       "   0.49063757061958313,\n",
       "   0.5196006894111633,\n",
       "   0.46588170528411865,\n",
       "   0.4708947241306305,\n",
       "   0.4721260964870453,\n",
       "   0.49853694438934326],\n",
       "  [0.46692249178886414,\n",
       "   0.5220759510993958,\n",
       "   0.5048929452896118,\n",
       "   0.5288379788398743,\n",
       "   0.4898432195186615,\n",
       "   0.5277711153030396,\n",
       "   0.4598439633846283,\n",
       "   0.46979770064353943,\n",
       "   0.4605463743209839,\n",
       "   0.4996195435523987],\n",
       "  [0.44905009865760803,\n",
       "   0.52361661195755,\n",
       "   0.5140159130096436,\n",
       "   0.5291934609413147,\n",
       "   0.4894247353076935,\n",
       "   0.5352762937545776,\n",
       "   0.45834168791770935,\n",
       "   0.4670118987560272,\n",
       "   0.44874122738838196,\n",
       "   0.496447890996933],\n",
       "  [0.42665520310401917,\n",
       "   0.5251380205154419,\n",
       "   0.5297182202339172,\n",
       "   0.5279824733734131,\n",
       "   0.48798325657844543,\n",
       "   0.5414242148399353,\n",
       "   0.4594918489456177,\n",
       "   0.46260207891464233,\n",
       "   0.43408021330833435,\n",
       "   0.49125513434410095],\n",
       "  [0.4031164348125458,\n",
       "   0.5257107615470886,\n",
       "   0.5459632873535156,\n",
       "   0.5304495096206665,\n",
       "   0.4891797602176666,\n",
       "   0.5490188598632812,\n",
       "   0.4588281512260437,\n",
       "   0.44747695326805115,\n",
       "   0.4023078680038452,\n",
       "   0.4877915680408478],\n",
       "  [0.3729502260684967,\n",
       "   0.5255358219146729,\n",
       "   0.5609372854232788,\n",
       "   0.5321497917175293,\n",
       "   0.48992788791656494,\n",
       "   0.5642527341842651,\n",
       "   0.46293866634368896,\n",
       "   0.4378302991390228,\n",
       "   0.3744458854198456,\n",
       "   0.4855373203754425]],\n",
       " [[0.4935073256492615,\n",
       "   0.5066367387771606,\n",
       "   0.5006406307220459,\n",
       "   0.4931355118751526,\n",
       "   0.5011196732521057,\n",
       "   0.5027900338172913,\n",
       "   0.5059390664100647,\n",
       "   0.5037103891372681,\n",
       "   0.5045592188835144,\n",
       "   0.49498361349105835],\n",
       "  [0.4864151179790497,\n",
       "   0.5134059190750122,\n",
       "   0.5013522505760193,\n",
       "   0.4798593819141388,\n",
       "   0.5054095983505249,\n",
       "   0.503986120223999,\n",
       "   0.5183268785476685,\n",
       "   0.5136097073554993,\n",
       "   0.5136396884918213,\n",
       "   0.4860149323940277],\n",
       "  [0.4804767370223999,\n",
       "   0.5187111496925354,\n",
       "   0.5010325312614441,\n",
       "   0.4613550305366516,\n",
       "   0.5138576626777649,\n",
       "   0.5021450519561768,\n",
       "   0.5373328924179077,\n",
       "   0.5301012396812439,\n",
       "   0.5266373753547668,\n",
       "   0.4751938283443451],\n",
       "  [0.4786783754825592,\n",
       "   0.5192803740501404,\n",
       "   0.49868643283843994,\n",
       "   0.4485275149345398,\n",
       "   0.5217024087905884,\n",
       "   0.4942969083786011,\n",
       "   0.5571663975715637,\n",
       "   0.548382043838501,\n",
       "   0.5394854545593262,\n",
       "   0.4674091041088104],\n",
       "  [0.4800170063972473,\n",
       "   0.5085130929946899,\n",
       "   0.4956192672252655,\n",
       "   0.44785237312316895,\n",
       "   0.5255739688873291,\n",
       "   0.4887203872203827,\n",
       "   0.5711811184883118,\n",
       "   0.5651856660842896,\n",
       "   0.5490503311157227,\n",
       "   0.46478331089019775],\n",
       "  [0.48153260350227356,\n",
       "   0.49942296743392944,\n",
       "   0.4930483400821686,\n",
       "   0.4832015037536621,\n",
       "   0.5232486724853516,\n",
       "   0.47565048933029175,\n",
       "   0.5736194252967834,\n",
       "   0.5832151174545288,\n",
       "   0.5526918172836304,\n",
       "   0.46285244822502136],\n",
       "  [0.4864833652973175,\n",
       "   0.48481810092926025,\n",
       "   0.48882997035980225,\n",
       "   0.5194024443626404,\n",
       "   0.513222873210907,\n",
       "   0.46280092000961304,\n",
       "   0.5727747678756714,\n",
       "   0.5997238159179688,\n",
       "   0.5543747544288635,\n",
       "   0.45990189909935],\n",
       "  [0.4895656704902649,\n",
       "   0.4684159457683563,\n",
       "   0.4830937385559082,\n",
       "   0.5518429279327393,\n",
       "   0.4951283931732178,\n",
       "   0.4554482400417328,\n",
       "   0.5680369734764099,\n",
       "   0.6143181324005127,\n",
       "   0.5561201572418213,\n",
       "   0.4577709138393402],\n",
       "  [0.4911474585533142,\n",
       "   0.4511832296848297,\n",
       "   0.4759911000728607,\n",
       "   0.5732237100601196,\n",
       "   0.48121845722198486,\n",
       "   0.4484676420688629,\n",
       "   0.5565111637115479,\n",
       "   0.6265982389450073,\n",
       "   0.5558149814605713,\n",
       "   0.4528338611125946],\n",
       "  [0.4923345148563385,\n",
       "   0.4308885931968689,\n",
       "   0.4694708287715912,\n",
       "   0.5909002423286438,\n",
       "   0.46782487630844116,\n",
       "   0.4447915256023407,\n",
       "   0.5333371758460999,\n",
       "   0.6277155876159668,\n",
       "   0.553862452507019,\n",
       "   0.4495563507080078]],\n",
       " [[0.49599671363830566,\n",
       "   0.505977988243103,\n",
       "   0.49741360545158386,\n",
       "   0.5000333189964294,\n",
       "   0.498515248298645,\n",
       "   0.5003623366355896,\n",
       "   0.5020152926445007,\n",
       "   0.4968193471431732,\n",
       "   0.5021724104881287,\n",
       "   0.49825751781463623],\n",
       "  [0.4982739984989166,\n",
       "   0.5114309191703796,\n",
       "   0.4927102029323578,\n",
       "   0.5087923407554626,\n",
       "   0.4982874393463135,\n",
       "   0.5002113580703735,\n",
       "   0.4950408637523651,\n",
       "   0.49080711603164673,\n",
       "   0.4985194206237793,\n",
       "   0.495974600315094],\n",
       "  [0.495180606842041,\n",
       "   0.5128109455108643,\n",
       "   0.49134019017219543,\n",
       "   0.5186378359794617,\n",
       "   0.4964984357357025,\n",
       "   0.505379319190979,\n",
       "   0.4830396771430969,\n",
       "   0.4827333390712738,\n",
       "   0.4921058714389801,\n",
       "   0.49838364124298096],\n",
       "  [0.48699524998664856,\n",
       "   0.514747679233551,\n",
       "   0.4969709813594818,\n",
       "   0.5244477987289429,\n",
       "   0.5002292394638062,\n",
       "   0.5135923027992249,\n",
       "   0.47346407175064087,\n",
       "   0.4732531011104584,\n",
       "   0.47692635655403137,\n",
       "   0.5020098090171814],\n",
       "  [0.4732399582862854,\n",
       "   0.5144741535186768,\n",
       "   0.501699686050415,\n",
       "   0.5289419889450073,\n",
       "   0.5003151893615723,\n",
       "   0.5248336791992188,\n",
       "   0.46726882457733154,\n",
       "   0.46334242820739746,\n",
       "   0.4561832547187805,\n",
       "   0.5071861743927002],\n",
       "  [0.44964221119880676,\n",
       "   0.5158843994140625,\n",
       "   0.5079138875007629,\n",
       "   0.5293943881988525,\n",
       "   0.4989471137523651,\n",
       "   0.5415902733802795,\n",
       "   0.4693746268749237,\n",
       "   0.45465245842933655,\n",
       "   0.43583324551582336,\n",
       "   0.5127857327461243],\n",
       "  [0.4221138060092926,\n",
       "   0.5176545977592468,\n",
       "   0.5147653222084045,\n",
       "   0.5295987129211426,\n",
       "   0.49693167209625244,\n",
       "   0.5577367544174194,\n",
       "   0.47876542806625366,\n",
       "   0.4452420175075531,\n",
       "   0.41869810223579407,\n",
       "   0.5082810521125793],\n",
       "  [0.38811028003692627,\n",
       "   0.5214492082595825,\n",
       "   0.5184506177902222,\n",
       "   0.5253399014472961,\n",
       "   0.49174442887306213,\n",
       "   0.5693498253822327,\n",
       "   0.5011019706726074,\n",
       "   0.45353057980537415,\n",
       "   0.4164471924304962,\n",
       "   0.4968683123588562],\n",
       "  [0.36234354972839355,\n",
       "   0.5278410911560059,\n",
       "   0.5153379440307617,\n",
       "   0.5202239751815796,\n",
       "   0.48276323080062866,\n",
       "   0.572359561920166,\n",
       "   0.5301076769828796,\n",
       "   0.4818229079246521,\n",
       "   0.4330885708332062,\n",
       "   0.4801550805568695],\n",
       "  [0.35057127475738525,\n",
       "   0.5377871990203857,\n",
       "   0.5082552433013916,\n",
       "   0.5131154656410217,\n",
       "   0.4674236476421356,\n",
       "   0.5599708557128906,\n",
       "   0.5598285794258118,\n",
       "   0.533362865447998,\n",
       "   0.4811905324459076,\n",
       "   0.4621477723121643]],\n",
       " [[0.49599671363830566,\n",
       "   0.505977988243103,\n",
       "   0.49741360545158386,\n",
       "   0.5000333189964294,\n",
       "   0.498515248298645,\n",
       "   0.5003623366355896,\n",
       "   0.5020152926445007,\n",
       "   0.4968193471431732,\n",
       "   0.5021724104881287,\n",
       "   0.49825751781463623],\n",
       "  [0.4982739984989166,\n",
       "   0.5114309191703796,\n",
       "   0.4927102029323578,\n",
       "   0.5087923407554626,\n",
       "   0.4982874393463135,\n",
       "   0.5002113580703735,\n",
       "   0.4950408637523651,\n",
       "   0.49080711603164673,\n",
       "   0.4985194206237793,\n",
       "   0.495974600315094],\n",
       "  [0.495180606842041,\n",
       "   0.5128109455108643,\n",
       "   0.49134019017219543,\n",
       "   0.5186378359794617,\n",
       "   0.4964984357357025,\n",
       "   0.505379319190979,\n",
       "   0.4830396771430969,\n",
       "   0.4827333390712738,\n",
       "   0.4921058714389801,\n",
       "   0.49838364124298096],\n",
       "  [0.48699524998664856,\n",
       "   0.514747679233551,\n",
       "   0.4969709813594818,\n",
       "   0.5244477987289429,\n",
       "   0.5002292394638062,\n",
       "   0.5135923027992249,\n",
       "   0.47346407175064087,\n",
       "   0.4732531011104584,\n",
       "   0.47692635655403137,\n",
       "   0.5020098090171814],\n",
       "  [0.47155246138572693,\n",
       "   0.5163626074790955,\n",
       "   0.5073354840278625,\n",
       "   0.5284625291824341,\n",
       "   0.5036620497703552,\n",
       "   0.5252302885055542,\n",
       "   0.4685995578765869,\n",
       "   0.4637703597545624,\n",
       "   0.4550075829029083,\n",
       "   0.505318284034729],\n",
       "  [0.44791990518569946,\n",
       "   0.5157428979873657,\n",
       "   0.5143195390701294,\n",
       "   0.5320842862129211,\n",
       "   0.5027382373809814,\n",
       "   0.5384891629219055,\n",
       "   0.46804317831993103,\n",
       "   0.4545551836490631,\n",
       "   0.4287174344062805,\n",
       "   0.509219229221344],\n",
       "  [0.4274927079677582,\n",
       "   0.5203395485877991,\n",
       "   0.5206096172332764,\n",
       "   0.5363749265670776,\n",
       "   0.4941402077674866,\n",
       "   0.5440685153007507,\n",
       "   0.47415691614151,\n",
       "   0.456539124250412,\n",
       "   0.4133474826812744,\n",
       "   0.49973049759864807],\n",
       "  [0.41368141770362854,\n",
       "   0.5285932421684265,\n",
       "   0.5237830281257629,\n",
       "   0.5407341122627258,\n",
       "   0.47680577635765076,\n",
       "   0.5372069478034973,\n",
       "   0.4789503216743469,\n",
       "   0.47505199909210205,\n",
       "   0.40986666083335876,\n",
       "   0.4800032675266266],\n",
       "  [0.40893372893333435,\n",
       "   0.540021538734436,\n",
       "   0.5230647921562195,\n",
       "   0.5466728806495667,\n",
       "   0.453422874212265,\n",
       "   0.5180574059486389,\n",
       "   0.4767017364501953,\n",
       "   0.5112937092781067,\n",
       "   0.4170326292514801,\n",
       "   0.4565376937389374],\n",
       "  [0.412627249956131,\n",
       "   0.5536556839942932,\n",
       "   0.5195404291152954,\n",
       "   0.5547642111778259,\n",
       "   0.4317971467971802,\n",
       "   0.49344614148139954,\n",
       "   0.4625207781791687,\n",
       "   0.5525162220001221,\n",
       "   0.4331713318824768,\n",
       "   0.43483832478523254]],\n",
       " [[0.4935073256492615,\n",
       "   0.5066367387771606,\n",
       "   0.5006406307220459,\n",
       "   0.4931355118751526,\n",
       "   0.5011196732521057,\n",
       "   0.5027900338172913,\n",
       "   0.5059390664100647,\n",
       "   0.5037103891372681,\n",
       "   0.5045592188835144,\n",
       "   0.49498361349105835],\n",
       "  [0.4888042211532593,\n",
       "   0.5124666690826416,\n",
       "   0.49886104464530945,\n",
       "   0.48768845200538635,\n",
       "   0.502082884311676,\n",
       "   0.5015733242034912,\n",
       "   0.5139681100845337,\n",
       "   0.5072578191757202,\n",
       "   0.5110902190208435,\n",
       "   0.4893607795238495],\n",
       "  [0.4837706387042999,\n",
       "   0.5132308006286621,\n",
       "   0.4966180622577667,\n",
       "   0.4896356761455536,\n",
       "   0.5010759830474854,\n",
       "   0.5032262802124023,\n",
       "   0.5173490047454834,\n",
       "   0.5093849897384644,\n",
       "   0.5153816342353821,\n",
       "   0.48739975690841675],\n",
       "  [0.4796925485134125,\n",
       "   0.5129684805870056,\n",
       "   0.49323591589927673,\n",
       "   0.495869517326355,\n",
       "   0.501282811164856,\n",
       "   0.50560462474823,\n",
       "   0.5209391713142395,\n",
       "   0.5099912881851196,\n",
       "   0.5158118009567261,\n",
       "   0.4864618182182312],\n",
       "  [0.4783409535884857,\n",
       "   0.5181713700294495,\n",
       "   0.49011996388435364,\n",
       "   0.503265917301178,\n",
       "   0.49712297320365906,\n",
       "   0.5029231905937195,\n",
       "   0.5176870822906494,\n",
       "   0.5175051093101501,\n",
       "   0.5209943652153015,\n",
       "   0.4768555462360382],\n",
       "  [0.47424039244651794,\n",
       "   0.5219233632087708,\n",
       "   0.4868907630443573,\n",
       "   0.5113959312438965,\n",
       "   0.49551692605018616,\n",
       "   0.5024139285087585,\n",
       "   0.5127443671226501,\n",
       "   0.5239893794059753,\n",
       "   0.5231109261512756,\n",
       "   0.4731998145580292],\n",
       "  [0.4707843065261841,\n",
       "   0.5274010300636292,\n",
       "   0.4835318326950073,\n",
       "   0.5203237533569336,\n",
       "   0.4962611496448517,\n",
       "   0.4970822036266327,\n",
       "   0.5096275210380554,\n",
       "   0.527242124080658,\n",
       "   0.5329503417015076,\n",
       "   0.46647271513938904],\n",
       "  [0.4672967195510864,\n",
       "   0.5302364826202393,\n",
       "   0.4804839491844177,\n",
       "   0.525964617729187,\n",
       "   0.49858424067497253,\n",
       "   0.4899025857448578,\n",
       "   0.5045422911643982,\n",
       "   0.5310878157615662,\n",
       "   0.5402598977088928,\n",
       "   0.4598078727722168],\n",
       "  [0.46306389570236206,\n",
       "   0.5290700197219849,\n",
       "   0.4814821779727936,\n",
       "   0.529658317565918,\n",
       "   0.49663886427879333,\n",
       "   0.48454758524894714,\n",
       "   0.49519726634025574,\n",
       "   0.5276616215705872,\n",
       "   0.5427936315536499,\n",
       "   0.45509234070777893],\n",
       "  [0.4558480978012085,\n",
       "   0.53023362159729,\n",
       "   0.4821963608264923,\n",
       "   0.5352409482002258,\n",
       "   0.4964573383331299,\n",
       "   0.4807736575603485,\n",
       "   0.4828425943851471,\n",
       "   0.5261974334716797,\n",
       "   0.5431912541389465,\n",
       "   0.4511813819408417]],\n",
       " [[0.49599671363830566,\n",
       "   0.505977988243103,\n",
       "   0.49741360545158386,\n",
       "   0.5000333189964294,\n",
       "   0.498515248298645,\n",
       "   0.5003623366355896,\n",
       "   0.5020152926445007,\n",
       "   0.4968193471431732,\n",
       "   0.5021724104881287,\n",
       "   0.49825751781463623],\n",
       "  [0.4982739984989166,\n",
       "   0.5114309191703796,\n",
       "   0.4927102029323578,\n",
       "   0.5087923407554626,\n",
       "   0.4982874393463135,\n",
       "   0.5002113580703735,\n",
       "   0.4950408637523651,\n",
       "   0.49080711603164673,\n",
       "   0.4985194206237793,\n",
       "   0.495974600315094],\n",
       "  [0.495180606842041,\n",
       "   0.5128109455108643,\n",
       "   0.49134019017219543,\n",
       "   0.5186378359794617,\n",
       "   0.4964984357357025,\n",
       "   0.505379319190979,\n",
       "   0.4830396771430969,\n",
       "   0.4827333390712738,\n",
       "   0.4921058714389801,\n",
       "   0.49838364124298096],\n",
       "  [0.488839328289032,\n",
       "   0.5130492448806763,\n",
       "   0.4918130338191986,\n",
       "   0.5242824554443359,\n",
       "   0.4961698055267334,\n",
       "   0.5140914916992188,\n",
       "   0.47307127714157104,\n",
       "   0.47317343950271606,\n",
       "   0.4796505272388458,\n",
       "   0.5027008652687073],\n",
       "  [0.47480571269989014,\n",
       "   0.5146147012710571,\n",
       "   0.49649596214294434,\n",
       "   0.5256413817405701,\n",
       "   0.4959128499031067,\n",
       "   0.5280333161354065,\n",
       "   0.4700377285480499,\n",
       "   0.46416711807250977,\n",
       "   0.4655117690563202,\n",
       "   0.5094239115715027],\n",
       "  [0.4534894526004791,\n",
       "   0.5187195539474487,\n",
       "   0.5011202096939087,\n",
       "   0.5259437561035156,\n",
       "   0.49559882283210754,\n",
       "   0.5434014797210693,\n",
       "   0.4788511395454407,\n",
       "   0.45399218797683716,\n",
       "   0.4594345688819885,\n",
       "   0.509539783000946],\n",
       "  [0.4300541579723358,\n",
       "   0.5228154063224792,\n",
       "   0.5059940814971924,\n",
       "   0.5240729451179504,\n",
       "   0.4956946074962616,\n",
       "   0.5538676977157593,\n",
       "   0.493369996547699,\n",
       "   0.4469212293624878,\n",
       "   0.45663511753082275,\n",
       "   0.4996611475944519],\n",
       "  [0.4003496468067169,\n",
       "   0.5280584096908569,\n",
       "   0.5086494088172913,\n",
       "   0.5175948143005371,\n",
       "   0.4922581613063812,\n",
       "   0.5592440366744995,\n",
       "   0.5186375975608826,\n",
       "   0.4586865305900574,\n",
       "   0.46907535195350647,\n",
       "   0.4862392842769623],\n",
       "  [0.3774715065956116,\n",
       "   0.535921037197113,\n",
       "   0.5061038136482239,\n",
       "   0.5092968940734863,\n",
       "   0.4850473701953888,\n",
       "   0.5549939870834351,\n",
       "   0.5482835173606873,\n",
       "   0.4893377721309662,\n",
       "   0.5029461979866028,\n",
       "   0.47050967812538147],\n",
       "  [0.3672855794429779,\n",
       "   0.5468208193778992,\n",
       "   0.501282811164856,\n",
       "   0.49827826023101807,\n",
       "   0.4723666310310364,\n",
       "   0.535592257976532,\n",
       "   0.5762588977813721,\n",
       "   0.5382131934165955,\n",
       "   0.5526970624923706,\n",
       "   0.45590513944625854]]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmodel.predict(input_data_[:10,:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 tf_venv shared",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
