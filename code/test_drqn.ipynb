{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import drqn\n",
    "import dataset_utils as d_utils\n",
    "import utils\n",
    "import models_dict_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data for DRQN\n",
    "We take the data from data generator and save them into traces of (s,a,r,sp) tuples.\n",
    "\n",
    "Each trajectory corresponds to a trace.\n",
    "\n",
    "If trajectory has length n, then trace will have length n-1. (since we need the next state sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = d_utils.load_data(filename=\"../synthetic_data/test-n10000-l3-random.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn_data = d_utils.preprocess_data_for_dqn(data, reward_model=\"dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]), array([ 1.,  0.,  0.,  0.,  0.]), 0.20000000000000001, array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  1.]), 0.20000000000000001, array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])]]\n"
     ]
    }
   ],
   "source": [
    "# Single Trace\n",
    "print (dqn_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "[ 1.  0.  0.  0.  0.]\n",
      "0.2\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# First tuple in a trace\n",
    "s,a,r,sp = dqn_data[0][0]\n",
    "print (s)\n",
    "print (a)\n",
    "print (r)\n",
    "print (sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.]\n",
      "0.2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Last tuple\n",
    "s,a,r,sp = dqn_data[0][-1]\n",
    "print (s)\n",
    "print (a)\n",
    "print (r)\n",
    "print (sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn_data_train, dqn_data_test = train_test_split(dqn_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DRQN model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"test_model_drqn\"\n",
    "\n",
    "# Directory for storing tensorboard summaries\n",
    "tensorboard_dir = '../tensorboard_logs/' + model_id + '/'\n",
    "summary_interval = 100\n",
    "checkpoint_dir = '../checkpoints/' + model_id + '/'\n",
    "checkpoint_path = checkpoint_dir + '_/'\n",
    "\n",
    "utils.check_if_path_exists_or_create(tensorboard_dir)\n",
    "utils.check_if_path_exists_or_create(checkpoint_dir)\n",
    "    \n",
    "checkpoint_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drqn_model = drqn.DRQNModel(model_id=model_id, timesteps=2)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "# writer_summary = tf.summary.FileWriter\n",
    "# histogram_summary = tf.summary.histogram\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    train_buffer = drqn.ExperienceBuffer()\n",
    "    train_buffer.buffer = dqn_data_train\n",
    "    train_buffer.buffer_sz = len(train_buffer.buffer)\n",
    "    drqn.train(drqn_model, session, saver,train_buffer,load_checkpoint=False, ckpt_path=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using tflearn Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 35999  | total loss: \u001b[1m\u001b[32m0.09022\u001b[0m\u001b[0m | time: 2.390s\n",
      "| Optimizer | epoch: 256 | loss: 0.09022 -- iter: 7936/8000\n",
      "Training Step: 36000  | total loss: \u001b[1m\u001b[32m0.08851\u001b[0m\u001b[0m | time: 3.414s\n",
      "| Optimizer | epoch: 256 | loss: 0.08851 | val_loss: 0.09024 -- iter: 8000/8000\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model_dict = models_dict_utils.load_model_dict(model_id)\n",
    "n_inputdim =model_dict[\"n_inputdim\"]\n",
    "n_hidden = model_dict[\"n_hidden\"]\n",
    "n_outputdim = model_dict[\"n_outputdim\"]\n",
    "graph_ops = drqn.build_tf_graph_drqn_tflearn(n_timesteps=2, n_inputdim=n_inputdim, n_hidden=n_hidden, n_actions=n_outputdim)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    train_buffer = drqn.ExperienceBuffer()\n",
    "    train_buffer.buffer = dqn_data_train\n",
    "    train_buffer.buffer_sz = len(train_buffer.buffer)\n",
    "    \n",
    "    val_buffer = drqn.ExperienceBuffer()\n",
    "    val_buffer.buffer = dqn_data_test\n",
    "    val_buffer.buffer_sz = len(val_buffer.buffer)\n",
    "\n",
    "    drqn.train_tflearn(graph_ops, train_buffer, val_buffer, n_epoch=256,\n",
    "                  tensorboard_dir=tensorboard_dir, run_id=\"test_run\", load_checkpoint=True,load_ckpt_path=checkpoint_dir, save_ckpt_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 tf_venv shared",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
